{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd685de0-a36f-4196-8527-cfcb911cbf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c09fe-0f02-4e82-a474-528f48a83808",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir('/home/dsxuser/work/train_Data/train')\n",
    "categories = []\n",
    "for filename in filenames:\n",
    "    category = filename.split(' ')[0]\n",
    "    if category == 'Iris Julian':\n",
    "        categories.append('Iris Julian')\n",
    "    elif category == 'Iris Oriana':\n",
    "        categories.append('Iris Oriana')\n",
    "    elif category == 'Iris Marcelo':\n",
    "        categories.append('Iris Marcelo')\n",
    "    else :\n",
    "        categories.append('Iris Desconocido')\n",
    "        \n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'category': categories\n",
    "})\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e704f0be-5d21-4e5f-8a8d-7045288b5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7c719-e925-42ae-a202-991658965fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfddc39c-b91d-412c-a224-539ba36f6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a979400-8e1e-4347-872b-22ff151ca776",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ebbf8e-60f5-45ad-b11f-b760a8a01283",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.choice(filenames)\n",
    "image = load_img(\"/home/dsxuser/work/train_Data/train/\"+sample)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ddce8-5067-477a-b1a2-faed32204db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validate_df = train_test_split(df, test_size=0.30, random_state=42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validate_df = validate_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b8c4b-9a05-447b-a2ca-dd70500301b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['category'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c8be9-6fc4-4df2-a91c-57873c192fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df['category'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6788870-6833-48d2-83fc-69fd6c850092",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train = train_df.shape[0]\n",
    "total_validate = validate_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06404d25-a7f2-44c5-9a2a-8b7a1425f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Add custom layers on top of ResNet50\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Reduce dimensionality and prevent overfitting\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)  # Helps to normalize the inputs of the activation layers\n",
    "x = Dropout(0.5)(x)  # Randomly sets input units to 0 at each step during training, which helps prevent overfitting\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "predictions = Dense(4, activation='softmax')(x)  # Final prediction layer with softmax for 4 classes\n",
    "\n",
    "# Finalizing the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1200d6f9-4fce-42c8-99e6-fee2de3dee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eedab99-28cb-4981-9e53-741e8efb93c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EarlyStopping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#To prevent over fitting we will stop the learning after 10 epochs and if val_loss value has not decreased.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m earlystop \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      4\u001b[0m learning_rate_reduction \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      5\u001b[0m                                             patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m      6\u001b[0m                                             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[0;32m      7\u001b[0m                                             factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, \n\u001b[0;32m      8\u001b[0m                                             min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m)\n\u001b[0;32m     10\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [earlystop, learning_rate_reduction]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EarlyStopping' is not defined"
     ]
    }
   ],
   "source": [
    "#Early Stop\n",
    "#To prevent over fitting we will stop the learning \n",
    "#after 10 epochs and if val_loss value has not decreased.\n",
    "earlystop = EarlyStopping(patience=10)\n",
    "\n",
    "#Learning Rate Reduction\n",
    "#We will reduce the learning rate when then accuracy does not increase for 2 steps\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "callbacks = [earlystop, learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fe124-a3ac-4c51-be39-527567550037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the image data generators for training and validation sets\n",
    "train_dir = './iris_data'  # CHANGE path to the training data directory\n",
    "batch_size = 32\n",
    "image_size = (224, 224)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0,\n",
    "    height_shift_range=0,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.5\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    train_dir,\n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_dataframe(\n",
    "    validate_df,\n",
    "    train_dir,\n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "epochs = 50  # Number of epochs to train for\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=epochs\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021033c7-3fee-4f50-8038-6b0ab5ba20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_Accuracy = model.evaluate_generator(generator=validation_generator, steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d376b-e631-410d-9570-24eef4e07401",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Model Accuracy for Validation data is: \", \"{:.2%}\".format(Validation_Accuracy[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ecc561-4d3b-4b11-8d34-6d9525ace9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "ax1.plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\n",
    "ax1.set_xticks(np.arange(1, epochs, 1))\n",
    "ax1.set_yticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "ax2.plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "ax2.plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n",
    "ax2.set_xticks(np.arange(1, epochs, 1))\n",
    "\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b1182-47b8-42c3-b2e6-0029b59ece92",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "LEFT_IRIS = [474, 475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "\n",
    "# Función principal\n",
    "def capture_iris_image():\n",
    "    cap = cv.VideoCapture(0)\n",
    "    captured_image = None\n",
    "    eye_detected_time = None\n",
    "    frame_with_contours = None\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame = cv.flip(frame, 1)\n",
    "            rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "            img_h, img_w = frame.shape[:2]\n",
    "            results = face_mesh.process(rgb_frame)\n",
    "            \n",
    "            if results.multi_face_landmarks:\n",
    "                mesh_points = np.array([np.multiply([p.x, p.y], [img_w, img_h]).astype(int) for p in results.multi_face_landmarks[0].landmark])\n",
    "                \n",
    "                (l_cx, l_cy), l_radius = cv.minEnclosingCircle(mesh_points[LEFT_IRIS])\n",
    "                center_left = np.array([l_cx, l_cy], dtype=np.int32)\n",
    "                (r_cx, r_cy), r_radius = cv.minEnclosingCircle(mesh_points[RIGHT_IRIS])\n",
    "                center_right = np.array([r_cx, r_cy], dtype=np.int32)\n",
    "                \n",
    "                frame_with_contours = frame.copy()\n",
    "                cv.circle(frame_with_contours, center_left, int(l_radius), (255, 0, 255), 1, cv.LINE_AA)\n",
    "                cv.circle(frame_with_contours, center_right, int(r_radius), (255, 0, 255), 1, cv.LINE_AA)\n",
    "\n",
    "                if eye_detected_time is None:\n",
    "                    eye_detected_time = time.time()  # Inicia el temporizador\n",
    "                elif time.time() - eye_detected_time >= 5:\n",
    "                    captured_image = frame.copy()  # Captura la imagen sin contornos\n",
    "                    break\n",
    "            else:\n",
    "                eye_detected_time = None  # Reinicia el temporizador si no se detectan ojos\n",
    "            \n",
    "            # Muestra el frame con contornos si están disponibles, de lo contrario muestra el frame original\n",
    "            if frame_with_contours is not None:\n",
    "                cv.imshow('img', frame_with_contours)\n",
    "            else:\n",
    "                cv.imshow('img', frame)\n",
    "            \n",
    "            key = cv.waitKey(1)\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "    return captured_image\n",
    "\n",
    "# Captura la imagen del iris\n",
    "captured_image = capture_iris_image()\n",
    "\n",
    "if captured_image is not None:\n",
    "    # Convierte la imagen capturada de BGR a RGB\n",
    "    captured_image_rgb = cv.cvtColor(captured_image, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Guarda la imagen como archivo\n",
    "    cv.imwrite('captured_iris_image.jpg', captured_image_rgb)\n",
    "    print(\"Imagen capturada y guardada exitosamente.\")\n",
    "    \n",
    "    # Plotea la imagen usando matplotlib\n",
    "    plt.imshow(captured_image_rgb)\n",
    "    plt.axis('off')  # Oculta los ejes\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No se capturó ninguna imagen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd02b4-a7ae-4ef0-ad5f-949f64bd3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess an image\n",
    "\n",
    "img = image.load_img(captured_image, target_size=image_size)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
    "img_preprocessed = img_array_expanded_dims / 255.0\n",
    "\n",
    "# Predict the class\n",
    "prediction = model.predict(img_preprocessed)\n",
    "predicted_class = np.argmax(prediction, axis=1)\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
